{"cells":[{"cell_type":"markdown","metadata":{"id":"sxb8_h-QFErO"},"source":["#2.&nbsp;Install TensorFlow Object Detection Dependencies"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!ls /content/drive/MyDrive/TFModel/uploaded_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEvZdnMKtWv0","executionInfo":{"status":"ok","timestamp":1722226704888,"user_tz":-570,"elapsed":27228,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"d39781ac-35bb-44fd-806b-8d0d00a488d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","new_model  output_model  output_model2\tpipeline_file.config  test01.jpg  train_again\n"]}]},{"cell_type":"markdown","metadata":{"id":"l7EOtpvlLeS0"},"source":["\n","\n","The latest version of TensorFlow this Colab has been verified to work with is TF v2.8.0.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypWGYdPlLRUN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722227567074,"user_tz":-570,"elapsed":2746,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"927883cb-fffc-4340-e957-6c8c92567958"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: Cython 3.0.10\n","Uninstalling Cython-3.0.10:\n","  Successfully uninstalled Cython-3.0.10\n"]}],"source":["# Clone the tensorflow models repository from GitHub\n","!pip uninstall Cython -y # Temporary fix for \"No module named 'object_detection'\" error\n","# !git clone --depth 1 https://github.com/tensorflow/models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QPmVBSlLTzM"},"outputs":[],"source":["# Copy setup files into models/research folder\n","%%bash\n","cd /content/drive/MyDrive/Jason/models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","#cp object_detection/packages/tf2/setup.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRBnuCKjM4Bd"},"outputs":[],"source":["# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n","import re\n","with open('/content/drive/MyDrive/Jason/models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()\n","\n","with open('/content/drive/MyDrive/Jason/models/research/setup.py', 'w') as f:\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('tf-models-official>=2.5.1',\n","               'tf-models-official==2.8.0', s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLDnCkLLwLr6"},"outputs":[],"source":["# Install the Object Detection API (NOTE: This block takes about 10 minutes to finish executing)\n","\n","# Need to do a temporary fix with PyYAML because Colab isn't able to install PyYAML v5.4.1\n","!pip install pyyaml==5.3\n","!pip install /content/drive/MyDrive/Jason/models/research/\n","\n","# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n","!pip install tensorflow==2.8.0\n","\n","# Install CUDA version 11.0 (to maintain compatibility with TF v2.8.0)\n","!pip install tensorflow_io==0.23.1\n","!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n","!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n","!apt-get update && sudo apt-get install cuda-toolkit-11-0\n","!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH\n","!pip install tensorflowjs\n"]},{"cell_type":"markdown","metadata":{"id":"6V7TrfUos-9E"},"source":["You may get warnings or errors related to package dependencies in the previous code block, but you can ignore them for now.\n","\n","Let's test our installation by running `model_builder_tf2_test.py` to make sure everything is working as expected. Run the following code block and confirm that it finishes without errors. If you get errors, try Googling them or checking the FAQ at the end of this Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh_HPMOqWH9z"},"outputs":[],"source":["# Run Model Bulider Test file, just to verify everything's working properly\n","!python /content/drive/MyDrive/Jason/models/research/object_detection/builders/model_builder_tf2_test.py\n"]},{"cell_type":"markdown","metadata":{"id":"eGEUZYAMEZ6f"},"source":["# 4.&nbsp;Set Up Training Configuration"]},{"cell_type":"markdown","metadata":{"id":"I2MAcgJ53STW"},"source":["In this section, we'll set up the model and training configuration. We'll specifiy which pretrained TensorFlow model we want to use from the [TensorFlow 2 Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Each model also comes with a configuration file that points to file locations, sets training parameters (such as learning rate and total number of training steps), and more. We'll modify the configuration file for our custom training job.\n","\n","The first section of code lists out some models availabe in the TF2 Model Zoo and defines some filenames that will be used later to download the model and config file. This makes it easy to manage which model you're using and to add other models to the list later.\n","\n","Set the \"chosen_model\" variable to match the name of the model you'd like to train with. It's currently set to use the popular \"ssd-mobilenet-v2\" model. Click play on the next block once the chosen model has been set.\n","\n","Not sure which model to pick? [Check out my blog post comparing each model's speed and accuracy.](https://ejtech.io/learn/tflite-object-detection-model-comparison)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN0EUEa3e5Un"},"outputs":[],"source":["# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n","chosen_model = 'ssd-mobilenet-v2'\n","\n","MODELS_CONFIG = {\n","    'ssd-mobilenet-v2': {\n","        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n","    },\n","    'efficientdet-d0': {\n","        'model_name': 'efficientdet_d0_coco17_tpu-32',\n","        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n","    },\n","    'ssd-mobilenet-v2-fpnlite-320': {\n","        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n","    },\n","    # The centernet model isn't working as of 9/10/22\n","    #'centernet-mobilenet-v2': {\n","    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n","    #    'base_pipeline_file': 'pipeline.config',\n","    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n","    #}\n","}\n","\n","model_name = MODELS_CONFIG[chosen_model]['model_name']\n","pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n","base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"]},{"cell_type":"markdown","metadata":{"id":"JMG3EEPqPggV"},"source":["Download the pretrained model file and configuration file by clicking Play on the following section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG4TmJUVrYQ7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722227779068,"user_tz":-570,"elapsed":4206,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"4524a18c-fc89-4cb9-da9e-83f81c3d86ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Jason/models/july29\n","--2024-07-29 04:36:14--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.127.207, 172.217.218.207, 142.251.31.207, ...\n","Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.127.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 46042990 (44M) [application/x-tar]\n","Saving to: ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’\n","\n","ssd_mobilenet_v2_32 100%[===================>]  43.91M  25.3MB/s    in 1.7s    \n","\n","2024-07-29 04:36:17 (25.3 MB/s) - ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’ saved [46042990/46042990]\n","\n","--2024-07-29 04:36:18--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4484 (4.4K) [text/plain]\n","Saving to: ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.config’\n","\n","ssd_mobilenet_v2_32 100%[===================>]   4.38K  --.-KB/s    in 0.004s  \n","\n","2024-07-29 04:36:18 (1.21 MB/s) - ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.config’ saved [4484/4484]\n","\n"]}],"source":["# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n","%mkdir /content/drive/MyDrive/Jason/models/july29/\n","%cd /content/drive/MyDrive/Jason/models/july29/\n","\n","# Download pre-trained model weights\n","import tarfile\n","download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n","!wget {download_tar}\n","tar = tarfile.open(pretrained_checkpoint)\n","tar.extractall()\n","tar.close()\n","\n","# Download training configuration file for model\n","download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n","!wget {download_config}"]},{"cell_type":"markdown","metadata":{"id":"BFAlqNrPn5y3"},"source":["Now that we've downloaded our model and config file, we need to modify the configuration file with some high-level training parameters. The following variables are used to control training steps:\n","\n","* **num_steps**: The total amount of steps to use for training the model. A good number to start with is 40,000 steps. You can use more steps if you notice the loss metrics are still decreasing by the time training finishes. The more steps, the longer training will take. Training can also be stopped early if loss flattens out before reaching the specified number of steps.\n","* **batch_size**: The number of images to use per training step. A larger batch size allows a model to be trained in fewer steps, but the size is limited by the GPU memory available for training. With the GPUs used in Colab instances, 16 is a good number for SSD models and 4 is good for EfficientDet models.\n","\n","Other training information, like the location of the pretrained model file, the config file, and total number of classes are also assigned in this step. To learn more about training configuration with the TensorFlow Object Detection API, read this [article from Neptune](https://neptune.ai/blog/tensorflow-object-detection-api-best-practices-to-training-evaluation-deployment)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lYDvJN-n69v"},"outputs":[],"source":["# Set training parameters for the model\n","# num_steps = 40000\n","num_steps = 2000\n","\n","if chosen_model == 'efficientdet-d0':\n","  batch_size = 4\n","else:\n","  batch_size = 16"]},{"cell_type":"markdown","metadata":{"id":"EXompElh9V26"},"source":["My images directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlccfa-o8Q2m"},"outputs":[],"source":["train_record_fname = '/content/drive/MyDrive/TFModel/4plants/vegs-fruits.tfrecord/train/vegs-fruits.tfrecord'\n","val_record_fname = '/content/drive/MyDrive/TFModel/4plants/vegs-fruits.tfrecord/valid/vegs-fruits.tfrecord'\n","label_map_pbtxt_fname = '/content/drive/MyDrive/TFModel/4plants/vegs-fruits.tfrecord/train/vegs-fruits_label_map.pbtxt'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4374,"status":"ok","timestamp":1722227828691,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"},"user_tz":-570},"id":"b_ki9jOqxn7V","outputId":"7400bf94-2a61-4234-d29f-31ae05885f9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total classes: 4\n"]}],"source":["# Set file locations and get number of classes for config file\n","pipeline_fname = '/content/drive/MyDrive/Jason/models/july29/' + base_pipeline_file\n","fine_tune_checkpoint = '/content/drive/MyDrive/Jason/models/july29/' + model_name + '/checkpoint/ckpt-0'\n","\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","print('Total classes:', num_classes)\n"]},{"cell_type":"markdown","metadata":{"id":"cwPyaIAXxyKu"},"source":["Next, we'll rewrite the config file to use the training parameters we just specified. The following section of code will automatically replace the necessary parameters in the downloaded .config file and save it as our custom \"pipeline_file.config\" file."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1722227876127,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"},"user_tz":-570},"id":"5eA5ht3_yukT","outputId":"62a418b7-e815-44eb-d351-d3b9783f36a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Jason/models/july29\n","writing custom configuration file\n"]}],"source":["# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n","import re\n","\n","%cd /content/drive/MyDrive/Jason/models/july29\n","print('writing custom configuration file')\n","\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open('pipeline_file.config', 'w') as f:\n","\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","\n","    # Set tfrecord files for train and test datasets\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n","\n","    # Set label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set batch_size\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","\n","    # Set number of classes num_classes\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","\n","    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n","    s = re.sub(\n","        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n","\n","    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n","    if chosen_model == 'ssd-mobilenet-v2':\n","      s = re.sub('learning_rate_base: .8',\n","                 'learning_rate_base: .08', s)\n","\n","      s = re.sub('warmup_learning_rate: 0.13333',\n","                 'warmup_learning_rate: .026666', s)\n","\n","    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n","    if chosen_model == 'efficientdet-d0':\n","      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n","      s = re.sub('pad_to_max_dimension: true', '', s)\n","      s = re.sub('min_dimension', 'height', s)\n","      s = re.sub('max_dimension', 'width', s)\n","\n","    f.write(s)\n"]},{"cell_type":"markdown","metadata":{"id":"GDySP7TLzdCM"},"source":["(Optional) If you're curious, you can display the configuration file's contents here in the browser by running the line of code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEsOLOMHzBqF"},"outputs":[],"source":["# (Optional) Display the custom configuration file's contents\n","!cat /content/drive/MyDrive/Jason/models/july29/pipeline_file.config"]},{"cell_type":"markdown","metadata":{"id":"UXpnXYC908Zl"},"source":["Finally, let's set the locations of the configuration file and model output directory as variables so we can reference them when we call the training command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMlaN3rs3zLe"},"outputs":[],"source":["# Set the path to the custom config file and the directory to store training checkpoints in\n","pipeline_file = '/content/drive/MyDrive/Jason/models/july29/pipeline_file.config'\n","model_dir = '/content/training/'"]},{"cell_type":"markdown","metadata":{"id":"-19zML6oEO7l"},"source":["# 5.&nbsp;Train Custom TFLite Detection Model"]},{"cell_type":"markdown","metadata":{"id":"XxPj_QV43qD5"},"source":["We're ready to train our object detection model! Before we start training, let's load up a TensorBoard session to monitor training progress. Run the following section of code, and a TensorBoard session will appear in the browser. It won't show anything yet, because we haven't started training. Once training starts, come back and click the refresh button to see the model's overall loss.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TI9iCCxoNlAL"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir '/content/training/train'"]},{"cell_type":"markdown","metadata":{"id":"5cuQpPJL2pUq"},"source":["Model training is performed using the \"model_main_tf2.py\" script from the TF Object Detection API. Training will take anywhere from 2 to 6 hours, depending on the model, batch size, and number of training steps. We've already defined all the parameters and arguments used by `model_main_tf2.py` in previous sections of this Colab. Just click Play on the following block to begin training!\n","\n","\n","\n","> *Note: It takes a few minutes for the program to display any training messages, because it only displays logs once every 100 steps. If it seems like nothing is happening, just wait a couple minutes.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQTfZChVzzpZ"},"outputs":[],"source":["# Run training!\n","!python /content/drive/MyDrive/Jason/models/research/object_detection/model_main_tf2.py \\\n","    --pipeline_config_path={pipeline_file} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --sample_1_of_n_eval_examples=1"]},{"cell_type":"markdown","metadata":{"id":"WHxbX4ZpzXIv"},"source":["If you want to stop training early, just click Stop a couple times or right-click on the code block and select \"Interrupt Execution\". Otherwise, training will stop by itself once it reaches the specified number of training steps.\n"]},{"cell_type":"markdown","metadata":{"id":"kPg8oMnQDYKl"},"source":["# 6.&nbsp;Convert Model to TensorFlow Lite"]},{"cell_type":"markdown","metadata":{"id":"spQXdq8Y63pj"},"source":["Alright! Our model is all trained up and ready to be used for detecting objects. First, we need to export the model graph (a file that contains information about the architecture and weights) to a TensorFlow Lite-compatible format. We'll do this using the `export_tflite_graph_tf2.py` script."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaUU8tBlHifd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722043703704,"user_tz":-570,"elapsed":7,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"39352c98-1caf-4fe1-ee44-f54c7cc1323f"},"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/models/research/object_detection/export_tflite_graph_tf2.py': [Errno 2] No such file or directory\n"]}],"source":["# Make a directory to store the trained TFLite model\n","!mkdir /content/custom_model_lite\n","output_directory = '/content/custom_model_lite'\n","\n","# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n","last_model_path = '/content/training'\n","\n","!python /content/drive/MyDrive/Jason/models/research/object_detection/export_tflite_graph_tf2.py \\\n","    --trained_checkpoint_dir {last_model_path} \\\n","    --output_directory {output_directory} \\\n","    --pipeline_config_path {pipeline_file}\n"]},{"cell_type":"markdown","metadata":{"id":"z_NuapO2VROu"},"source":["Next, we'll take the exported graph and use the `TFLiteConverter` module to convert it to `.tflite` FlatBuffer format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsE_uVjlsz3u"},"outputs":[],"source":["# Convert exported graph file into TFLite model file\n","import tensorflow as tf\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n","tflite_model = converter.convert()\n","\n","with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5h89kxEkezn"},"outputs":[],"source":["!cp /content/custom_model_lite/detect.tflite /content/drive/MyDrive/TFModel/4plants/vegs-fruits.record/detect.tflite\n","!mv /content/training /content/drive/MyDrive/TFModel/4plants/train_again\n","!cp /content/custom_model_lite/saved_model/saved_model.pb /content/drive/MyDrive/TFModel/4plants/vegs-fruits.record/saved_model.pd\n"]},{"cell_type":"code","source":["!mv /content/custom_model_lite/saved_model /content/drive/MyDrive/TFModel/4plants/train_again2"],"metadata":{"id":"xCQXU4H1On7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDQrtQhvC3oG"},"source":["# 7.&nbsp;Test TensorFlow Lite Model and Calculate mAP"]},{"cell_type":"markdown","metadata":{"id":"vtSmUZcxIAvt"},"source":["We've trained our custom model and converted it to TFLite format. But how well does it actually perform at detecting objects in images? This is where the images we set aside in the **test** folder come in. The model never saw any test images during training, so its performance on these images should be representative of how it will perform on new images from the field.\n","\n","### 7.1 Inference test images\n","The following code defines a function to run inference on test images. It loads the images, loads the model and labelmap, runs the model on each image, and displays the result. It also optionally saves detection results as text files so we can use them to calculate model mAP score.\n","\n","This code is based off the [TFLite_detection_image.py](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py) script from my [TensorFlow Lite Object Detection repository on GitHub](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi); feel free to use it as a starting point for your own application."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4WtI8i5K96w"},"outputs":[],"source":["# Script to run custom TFLite model on test images to detect objects\n","# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n","\n","# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","import time\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n","\n","  # Grab filenames of all images in test folder\n","  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","  # Load the label map into memory\n","  with open(lblpath, 'r') as f:\n","      labels = [line.strip() for line in f.readlines()]\n","\n","  # Load the Tensorflow Lite model into memory\n","  interpreter = Interpreter(model_path=modelpath)\n","  interpreter.allocate_tensors()\n","\n","  # Get model details\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","\n","  float_input = (input_details[0]['dtype'] == np.float32)\n","\n","  input_mean = 127.5\n","  input_std = 127.5\n","\n","  # Randomly select test images\n","  images_to_test = random.sample(images, num_test_images)\n","\n","  # Loop over every image and perform detection\n","  for image_path in images_to_test:\n","      start_time = time.time()\n","\n","      # Load image and resize to expected shape [1xHxWx3]\n","      image = cv2.imread(image_path)\n","      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      imH, imW, _ = image.shape\n","      image_resized = cv2.resize(image_rgb, (width, height))\n","      input_data = np.expand_dims(image_resized, axis=0)\n","\n","      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","      if float_input:\n","          input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","      # Perform the actual detection by running the model with the image as input\n","      interpreter.set_tensor(input_details[0]['index'],input_data)\n","      interpreter.invoke()\n","\n","      # Retrieve detection results\n","      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","      detections = []\n","\n","      # Loop over all detections and draw detection box if confidence is above minimum threshold\n","      for i in range(len(scores)):\n","          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","              # Get bounding box coordinates and draw box\n","              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","              ymin = int(max(1,(boxes[i][0] * imH)))\n","              xmin = int(max(1,(boxes[i][1] * imW)))\n","              ymax = int(min(imH,(boxes[i][2] * imH)))\n","              xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","              # Draw label\n","              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n","              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n","\n","              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n","\n","\n","      # All the results have been drawn on the image, now display the image\n","      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","        plt.figure(figsize=(12,16))\n","        plt.imshow(image)\n","        plt.show()\n","\n","      # Save detection results in .txt files (for calculating mAP)\n","      elif txt_only == True:\n","\n","        # Get filenames and paths\n","        image_fn = os.path.basename(image_path)\n","        base_fn, ext = os.path.splitext(image_fn)\n","        txt_result_fn = base_fn +'.txt'\n","        txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","        # Write results to text file\n","        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n","        with open(txt_savepath,'w') as f:\n","            for detection in detections:\n","                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","      end_time = time.time()\n","      elapsed_time = end_time - start_time\n","      print(f\"Time taken for {os.path.basename(image_path)}: {elapsed_time:.2f} seconds\")\n","\n","  return\n"]},{"cell_type":"markdown","metadata":{"id":"-CJI4A0f_zqz"},"source":["The next block sets the paths to the test images and models and then runs the inferencing function. If you want to use more than 10 images, change the `images_to_test` variable. Click play to run inferencing!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6t8CMarqBqP9"},"outputs":[],"source":["# Set up variables for running user's model\n","PATH_TO_IMAGES='/content/drive/MyDrive/TFModel/4plants/valid'   # Path to test images folder\n","PATH_TO_MODEL='/content/drive/MyDrive/TFModel/detect.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/drive/MyDrive/TFModel/4plants/vegs-fruits.tfrecord/label.txt'   # Path to labelmap.txt file\n","min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n","images_to_test = 10   # Number of images to run detection on\n","\n","# Run inferencing function!\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"]},{"cell_type":"markdown","metadata":{"id":"N_ckqeWqBF0P"},"source":["### 7.2 Calculate mAP\n","Now we have a visual sense of how our model performs on test images, but how can we quantitatively measure its accuracy?\n","\n","One popular methord for measuring object detection model accuracy is \"mean average precision\" (mAP). Basically, the higher the mAP score, the better your model is at detecting objects in images. To learn more about mAP, read through this [article from Roboflow](https://blog.roboflow.com/mean-average-precision/).\n","\n","We'll use the mAP calculator tool at https://github.com/Cartucho/mAP to determine our model's mAP score. First, we need to clone the repository and remove its existing example data. We'll also download a script I wrote for interfacing with the calculator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlWarXEZDUqS"},"outputs":[],"source":["%%bash\n","git clone https://github.com/Cartucho/mAP /content/mAP\n","cd /content/mAP\n","rm input/detection-results/*\n","rm input/ground-truth/*\n","rm input/images-optional/*\n","wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"]},{"cell_type":"markdown","metadata":{"id":"qn22nGGqH5T6"},"source":["Next, we'll copy the images and annotation data from the **test** folder to the appropriate folders inside the cloned repository. These will be used as the \"ground truth data\" that our model's detection results will be compared to.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5szFfVxwI3wT"},"outputs":[],"source":["!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n","!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"]},{"cell_type":"markdown","metadata":{"id":"u6aro817DGzx"},"source":["The calculator tool expects annotation data in a format that's different from the Pascal VOC .xml file format we're using. Fortunately, it provides an easy script, `convert_gt_xml.py`, for converting to the expected .txt format.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdjtOUDnK2AA"},"outputs":[],"source":["!python /content/mAP/scripts/extra/convert_gt_xml.py"]},{"cell_type":"markdown","metadata":{"id":"mnIUacAlLP0B"},"source":["Okay, we've set up the ground truth data, but now we need actual detection results from our model. The detection results will be compared to the ground truth data to calculate the model's accuracy in mAP.\n","\n","The inference function we defined in Step 7.1 can be used to generate detection data for all the images in the **test** folder. We'll use it the same as before, except this time we'll tell it to save detection results into the `detection-results` folder.\n","\n","Click Play to run the following code block!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szzHFAhsMNFF"},"outputs":[],"source":["# Set up variables for running inference, this time to get detection results saved as .txt files\n","PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n","PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n","min_conf_threshold=0.1   # Confidence threshold\n","\n","# Use all the images in the test folder\n","image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n","images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n","\n","# Tell function to just save results and not display images\n","txt_only = True\n","\n","# Run inferencing function!\n","print('Starting inference on %d images...' % images_to_test)\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n","print('Finished inferencing!')"]},{"cell_type":"markdown","metadata":{"id":"e_QRnTqNPX4z"},"source":["Finally, let's calculate mAP! One popular style for reporting mAP is the COCO metric for mAP @ 0.50:0.95. Basically, this means that mAP is calculated at several IoU thresholds between 0.50 and 0.95, and then the result from each threshold is averaged to get a final mAP score. [Learn more here!](https://blog.roboflow.com/mean-average-precision/)\n","\n","I wrote a script to run the calculator tool at each IoU threshold, average the results, and report the final accuracy score. It reports mAP for each class and overall mAP. Click Play on the following two blocks to calculate mAP!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DkjpIBARTQ7"},"outputs":[],"source":["%cd /content/mAP\n","!python calculate_map_cartucho.py --labels=/content/labelmap.txt"]},{"cell_type":"markdown","metadata":{"id":"R9HPoOBVKvxU"},"source":["The score reported at the end is your model's overall mAP score. Ideally, it should be above 50% (0.50). If it isn't, you can increase your model's accuracy by adding more images to your dataset. See my [dataset video](https://www.youtube.com/watch?v=v0ssiOY6cfg) for tips on how to capture good training images and improve accuracy."]},{"cell_type":"markdown","source":["convert to model.json"],"metadata":{"id":"ULgr2yIG6bdM"}},{"cell_type":"markdown","source":["# 8.&nbsp;Try Output\n"],"metadata":{"id":"KXaUKsjK83nA"}},{"cell_type":"code","source":["!tensorflowjs_converter \\\n","--input_format=tf_saved_model \\\n","--output_format=tfjs_graph_model \\\n","/content/drive/MyDrive/TFModel/july29/out_graph/saved_model \\\n","/content/drive/MyDrive/TFModel/july29/out_graph/graph_model"],"metadata":{"id":"SfaI496T6Wwd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722230581321,"user_tz":-570,"elapsed":31976,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"be8e1a49-1d3c-4598-99f0-89711c4259d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-29 05:22:29.640808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-29 05:22:29.665242: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-29 05:22:29.665296: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-29 05:22:30.663399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-29 05:22:33.183977: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n"]}]},{"cell_type":"code","source":["!python /content/drive/MyDrive/Jason/models/research/object_detection/exporter_main_v2.py \\\n","    --trained_checkpoint_dir '/content/training' \\\n","    --output_directory '/content/drive/MyDrive/TFModel/july29/out_graph' \\\n","    --pipeline_config_path '/content/drive/MyDrive/Jason/models/july29/pipeline_file.config'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBhrOAID4RlA","executionInfo":{"status":"ok","timestamp":1722230271028,"user_tz":-570,"elapsed":51333,"user":{"displayName":"Jiansong Feng (Jason)","userId":"03706139387887615462"}},"outputId":"12ef783e-1e84-41dd-9370-6fed70421e34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-07-29 05:17:04.358399: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","W0729 05:17:04.499925 137165426209408 deprecation.py:610] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","I0729 05:17:08.428894 137165426209408 api.py:441] feature_map_spatial_dims: [(19, 19), (10, 10), (5, 5), (3, 3), (2, 2), (1, 1)]\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.124291 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.124577 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.124744 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.124885 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.125016 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0729 05:17:11.125142 137165426209408 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n","I0729 05:17:18.736663 137165426209408 api.py:441] feature_map_spatial_dims: [(19, 19), (10, 10), (5, 5), (3, 3), (2, 2), (1, 1)]\n","2024-07-29 05:17:21.491118: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","I0729 05:17:22.089660 137165426209408 api.py:441] feature_map_spatial_dims: [(19, 19), (10, 10), (5, 5), (3, 3), (2, 2), (1, 1)]\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7cbfd3c646d0>, because it is not built.\n","W0729 05:17:24.017003 137165426209408 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7cbfd3c646d0>, because it is not built.\n","W0729 05:17:42.759423 137165426209408 save.py:260] Found untraced functions such as BoxPredictor_layer_call_fn, BoxPredictor_layer_call_and_return_conditional_losses, ConvolutionalBoxHead_0_layer_call_fn, ConvolutionalBoxHead_0_layer_call_and_return_conditional_losses, ConvolutionalBoxHead_1_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/TFModel/july29/out_graph/saved_model/assets\n","I0729 05:17:47.843228 137165426209408 builder_impl.py:779] Assets written to: /content/drive/MyDrive/TFModel/july29/out_graph/saved_model/assets\n","INFO:tensorflow:Writing pipeline config file to /content/drive/MyDrive/TFModel/july29/out_graph/pipeline.config\n","I0729 05:17:48.508904 137165426209408 config_util.py:253] Writing pipeline config file to /content/drive/MyDrive/TFModel/july29/out_graph/pipeline.config\n"]}]},{"cell_type":"code","source":["!python /content/drive/MyDrive/Jason/models/research/object_detection/export_inference_graph.py \\\n"," --input_type=image_tensor \\\n"," --pipeline_config_path=/content/drive/MyDrive/Jason/models/july29/pipeline_file.config \\\n"," --trained_checkpoint_prefix=/content/drive/MyDrive/Jason/models/july29/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001 \\\n"," --output_directory=/content/drive/MyDrive/Jason/model/july29/output"],"metadata":{"id":"8ZB0Deax18V1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sxb8_h-QFErO","eGEUZYAMEZ6f"],"gpuType":"T4","provenance":[{"file_id":"13h_W6G6-iyKVq6V43B6eZJbhbO2XYv1S","timestamp":1716720972795},{"file_id":"https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite2_Object_Detction_Model.ipynb","timestamp":1715319742237}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}